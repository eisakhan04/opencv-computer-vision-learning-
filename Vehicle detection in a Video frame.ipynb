{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6878b727-138d-4bef-8429-9c712e752bc5",
   "metadata": {},
   "source": [
    "# Vehicle detection in a Video frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce1c693-23ad-469d-8732-28ce158ed391",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Detect vehicles\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m cars \u001b[38;5;241m=\u001b[39m \u001b[43mcar_cascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaleFactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminNeighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Draw rectangles around the detected vehicles\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m cars:\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained Haar Cascade classifier for vehicles\n",
    "car_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_car.xml\")\n",
    "\n",
    "# Load the video\n",
    "video_path = r\"C:\\Users\\Mega Computers\\Downloads\\highway.mp4\"  # Replace with your video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video was successfully loaded\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open the video.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video or cannot read the frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect vehicles\n",
    "    cars = car_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=3, minSize=(50, 50))\n",
    "\n",
    "    # Draw rectangles around the detected vehicles\n",
    "    for (x, y, w, h) in cars:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green rectangle\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Vehicle Detection\", frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f97a914-4d42-4a8b-a46b-5f971fa400fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained Haar Cascade classifier for vehicles\n",
    "car_cascade = cv2.CascadeClassifier(r\"C:\\Users\\Mega Computers\\Downloads\\37e1e0af2bf8965e8058a9dfa3285bc6-e690cef3fb3ede5b869c3969cd6a9c5735d4ec7b (1)\\37e1e0af2bf8965e8058a9dfa3285bc6-e690cef3fb3ede5b869c3969cd6a9c5735d4ec7b\\cars.xml\")\n",
    "\n",
    "# Verify if the cascade file is loaded\n",
    "if car_cascade.empty():\n",
    "    print(\"Error: Haar Cascade file not loaded. Check the file path.\")\n",
    "    exit()\n",
    "\n",
    "# Load the video\n",
    "video_path = r\"C:\\Users\\Mega Computers\\Downloads\\highway.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video was successfully loaded\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open the video.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video or cannot read the frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect vehicles\n",
    "    cars = car_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=3, minSize=(50, 50))\n",
    "\n",
    "    # Draw rectangles around the detected vehicles\n",
    "    for (x, y, w, h) in cars:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green rectangle\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Vehicle Detection\", frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa4ecda-39bf-46a6-9ad4-8353146c0daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "981f4a5a-b658-4ebd-a14b-55dad12f02b9",
   "metadata": {},
   "source": [
    "# Smile and Eye detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13ec9ad7-779b-4289-b513-2367ff402881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load Haar Cascade classifiers for face, eyes, and smile\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_smile.xml\")\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for default webcam\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Cannot read the frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the detected face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "        # Define regions of interest for eyes and smiles within the detected face\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_color = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Detect eyes within the face region\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=10, minSize=(15, 15))\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
    "\n",
    "        # Detect smiles within the face region\n",
    "        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.7, minNeighbors=20, minSize=(25, 25))\n",
    "        for (sx, sy, sw, sh) in smiles:\n",
    "            cv2.rectangle(roi_color, (sx, sy), (sx + sw, sy + sh), (0, 0, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Smile and Eye Detection\", frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29287755-7aa9-4375-9760-ae42f488beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve code \n",
    "\n",
    "import cv2\n",
    "\n",
    "# Load Haar Cascade classifiers for face, eyes, and smile\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "smile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_smile.xml\")\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for default webcam\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Cannot read the frame.\")\n",
    "        break\n",
    "\n",
    "    # Resize the frame for faster processing (optional)\n",
    "    frame_resized = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame with improved parameters\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the detected face\n",
    "        cv2.rectangle(frame_resized, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "        # Define regions of interest for eyes and smiles within the detected face\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_color = frame_resized[y:y + h, x:x + w]\n",
    "\n",
    "        # Detect eyes within the face region with adjusted parameters\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=5, minSize=(20, 20))\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)\n",
    "\n",
    "        # Detect smiles within the face region with adjusted parameters\n",
    "        smiles = smile_cascade.detectMultiScale(roi_gray, scaleFactor=1.8, minNeighbors=25, minSize=(40, 40))\n",
    "        for (sx, sy, sw, sh) in smiles:\n",
    "            cv2.rectangle(roi_color, (sx, sy), (sx + sw, sy + sh), (0, 0, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Smile and Eye Detection\", frame_resized)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f54df53-2698-4255-ba11-0fa89014f392",
   "metadata": {},
   "source": [
    "# Pedestrian detection from a streaming video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c22c42b-27ca-4762-8334-ca4d566136d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize the HOG descriptor/person detector\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# Open the webcam or video stream (0 for default webcam)\n",
    "cap = cv2.VideoCapture(0)  # Use a file path here for a pre-recorded video\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot access the video stream.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Cannot read the frame or video has ended.\")\n",
    "        break\n",
    "\n",
    "    # Resize the frame for faster processing (optional)\n",
    "    frame_resized = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Detect pedestrians in the frame\n",
    "    # The detectMultiScale function returns rectangles of detected objects\n",
    "    pedestrians, weights = hog.detectMultiScale(frame_resized, winStride=(8, 8), padding=(8, 8), scale=1.05)\n",
    "\n",
    "    # Draw rectangles around detected pedestrians\n",
    "    for (x, y, w, h) in pedestrians:\n",
    "        cv2.rectangle(frame_resized, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green rectangle\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Pedestrian Detection\", frame_resized)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a944fcee-a44c-476b-8cf7-2fcfdabc6f0c",
   "metadata": {},
   "source": [
    "# MediaPipe (Face Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b3e97c5-586b-4c5b-853c-4961b85e96c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19d11c84-85a5-431f-9be6-bd87586dbfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\mega computers\\appdata\\roaming\\python\\python38\\site-packages (0.10.11)\n",
      "Requirement already satisfied: absl-py in c:\\users\\mega computers\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\mega computers\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (0.4.13)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from mediapipe) (3.7.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from mediapipe) (1.24.3)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\mega computers\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\mega computers\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\mega computers\\appdata\\roaming\\python\\python38\\site-packages (from jax->mediapipe) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from jax->mediapipe) (1.10.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from jax->mediapipe) (7.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (6.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4175d241-b876-4e42-bcb3-2d5ec8e98f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\mega computers\\appdata\\roaming\\python\\python38\\site-packages (0.10.11)\n",
      "Requirement already satisfied: absl-py in c:\\users\\mega computers\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\mega computers\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (0.4.13)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from mediapipe) (3.7.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from mediapipe) (1.24.3)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\mega computers\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\mega computers\\appdata\\roaming\\python\\python38\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\mega computers\\appdata\\roaming\\python\\python38\\site-packages (from jax->mediapipe) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from jax->mediapipe) (1.10.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from jax->mediapipe) (7.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from matplotlib->mediapipe) (6.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mega computers\\.conda\\envs\\opencv_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ae9693-ff5c-46b2-b92a-9515fc7a6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open webcam (use a video file path if needed)\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default camera\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the MediaPipe Face Detection model\n",
    "with mp_face_detection.FaceDetection(min_detection_confidence=0.5) as face_detection:\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Cannot read the frame.\")\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB (MediaPipe requires RGB format)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Perform face detection\n",
    "        results = face_detection.process(frame_rgb)\n",
    "\n",
    "        # Draw face detection results\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Draw bounding boxes and keypoints on the face\n",
    "                mp_drawing.draw_detection(frame, detection)\n",
    "\n",
    "        # Display the frame with detections\n",
    "        cv2.imshow(\"MediaPipe Face Detection\", frame)\n",
    "\n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b2de8ae-a45a-4676-bbdf-b02f7ca9fcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'conda' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cde3e-7a96-46b5-96ec-c3901d71cbff",
   "metadata": {},
   "source": [
    "# MediaPipe  Hand Tracking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6015a13f-6305-4781-b009-1c49d6f7e00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thumb Tip Position: [4, 550, 331]\n",
      "Thumb Tip Position: [4, 530, 191]\n",
      "Thumb Tip Position: [4, 465, 192]\n",
      "Thumb Tip Position: [4, 448, 199]\n",
      "Thumb Tip Position: [4, 410, 186]\n",
      "Thumb Tip Position: [4, 330, 153]\n",
      "Thumb Tip Position: [4, 254, 150]\n",
      "Thumb Tip Position: [4, 246, 165]\n",
      "Thumb Tip Position: [4, 257, 172]\n",
      "Thumb Tip Position: [4, 266, 182]\n",
      "Thumb Tip Position: [4, 274, 203]\n",
      "Thumb Tip Position: [4, 290, 206]\n",
      "Thumb Tip Position: [4, 258, 187]\n",
      "Thumb Tip Position: [4, 254, 188]\n",
      "Thumb Tip Position: [4, 252, 189]\n",
      "Thumb Tip Position: [4, 266, 192]\n",
      "Thumb Tip Position: [4, 279, 200]\n",
      "Thumb Tip Position: [4, 283, 201]\n",
      "Thumb Tip Position: [4, 288, 203]\n",
      "Thumb Tip Position: [4, 278, 198]\n",
      "Thumb Tip Position: [4, 249, 183]\n",
      "Thumb Tip Position: [4, 230, 190]\n",
      "Thumb Tip Position: [4, 229, 191]\n",
      "Thumb Tip Position: [4, 229, 191]\n",
      "Thumb Tip Position: [4, 238, 193]\n",
      "Thumb Tip Position: [4, 260, 197]\n",
      "Thumb Tip Position: [4, 217, 193]\n",
      "Thumb Tip Position: [4, 207, 203]\n",
      "Thumb Tip Position: [4, 209, 206]\n",
      "Thumb Tip Position: [4, 210, 205]\n",
      "Thumb Tip Position: [4, 209, 204]\n",
      "Thumb Tip Position: [4, 212, 206]\n",
      "Thumb Tip Position: [4, 251, 204]\n",
      "Thumb Tip Position: [4, 227, 211]\n",
      "Thumb Tip Position: [4, 224, 212]\n",
      "Thumb Tip Position: [4, 228, 213]\n",
      "Thumb Tip Position: [4, 250, 211]\n",
      "Thumb Tip Position: [4, 234, 229]\n",
      "Thumb Tip Position: [4, 281, 233]\n",
      "Thumb Tip Position: [4, 328, 241]\n",
      "Thumb Tip Position: [4, 345, 262]\n",
      "Thumb Tip Position: [4, 345, 267]\n",
      "Thumb Tip Position: [4, 345, 267]\n",
      "Thumb Tip Position: [4, 289, 223]\n",
      "Thumb Tip Position: [4, 257, 208]\n",
      "Thumb Tip Position: [4, 272, 214]\n",
      "Thumb Tip Position: [4, 273, 216]\n",
      "Thumb Tip Position: [4, 277, 216]\n",
      "Thumb Tip Position: [4, 283, 218]\n",
      "Thumb Tip Position: [4, 281, 217]\n",
      "Thumb Tip Position: [4, 281, 219]\n",
      "Thumb Tip Position: [4, 281, 220]\n",
      "Thumb Tip Position: [4, 279, 219]\n",
      "Thumb Tip Position: [4, 275, 217]\n",
      "Thumb Tip Position: [4, 274, 217]\n",
      "Thumb Tip Position: [4, 274, 217]\n",
      "Thumb Tip Position: [4, 275, 218]\n",
      "Thumb Tip Position: [4, 277, 217]\n",
      "Thumb Tip Position: [4, 273, 216]\n",
      "Thumb Tip Position: [4, 271, 216]\n",
      "Thumb Tip Position: [4, 266, 214]\n",
      "Thumb Tip Position: [4, 264, 215]\n",
      "Thumb Tip Position: [4, 273, 215]\n",
      "Thumb Tip Position: [4, 287, 225]\n",
      "Thumb Tip Position: [4, 293, 223]\n",
      "Thumb Tip Position: [4, 302, 234]\n",
      "Thumb Tip Position: [4, 299, 235]\n",
      "Thumb Tip Position: [4, 307, 237]\n",
      "Thumb Tip Position: [4, 314, 238]\n",
      "Thumb Tip Position: [4, 303, 237]\n",
      "Thumb Tip Position: [4, 299, 235]\n",
      "Thumb Tip Position: [4, 293, 231]\n",
      "Thumb Tip Position: [4, 282, 231]\n",
      "Thumb Tip Position: [4, 275, 228]\n",
      "Thumb Tip Position: [4, 284, 231]\n",
      "Thumb Tip Position: [4, 295, 236]\n",
      "Thumb Tip Position: [4, 296, 235]\n",
      "Thumb Tip Position: [4, 301, 243]\n",
      "Thumb Tip Position: [4, 328, 272]\n",
      "Thumb Tip Position: [4, 329, 273]\n",
      "Thumb Tip Position: [4, 324, 270]\n",
      "Thumb Tip Position: [4, 320, 264]\n",
      "Thumb Tip Position: [4, 301, 251]\n",
      "Thumb Tip Position: [4, 298, 257]\n",
      "Thumb Tip Position: [4, 295, 257]\n",
      "Thumb Tip Position: [4, 299, 258]\n",
      "Thumb Tip Position: [4, 282, 236]\n",
      "Thumb Tip Position: [4, 270, 225]\n",
      "Thumb Tip Position: [4, 253, 212]\n",
      "Thumb Tip Position: [4, 236, 205]\n",
      "Thumb Tip Position: [4, 226, 203]\n",
      "Thumb Tip Position: [4, 226, 203]\n",
      "Thumb Tip Position: [4, 225, 200]\n",
      "Thumb Tip Position: [4, 180, 192]\n",
      "Thumb Tip Position: [4, 97, 181]\n",
      "Thumb Tip Position: [4, 89, 195]\n",
      "Thumb Tip Position: [4, 103, 193]\n",
      "Thumb Tip Position: [4, 122, 207]\n",
      "Thumb Tip Position: [4, 42, 221]\n",
      "Thumb Tip Position: [4, 32, 229]\n",
      "Thumb Tip Position: [4, 34, 231]\n",
      "Thumb Tip Position: [4, 31, 230]\n",
      "Thumb Tip Position: [4, 33, 231]\n",
      "Thumb Tip Position: [4, 34, 231]\n",
      "Thumb Tip Position: [4, 37, 232]\n",
      "Thumb Tip Position: [4, 39, 232]\n",
      "Thumb Tip Position: [4, 37, 234]\n",
      "Thumb Tip Position: [4, 44, 230]\n",
      "Thumb Tip Position: [4, 44, 233]\n",
      "Thumb Tip Position: [4, 37, 233]\n",
      "Thumb Tip Position: [4, 30, 238]\n",
      "Thumb Tip Position: [4, 22, 242]\n",
      "Thumb Tip Position: [4, 6, 238]\n",
      "Thumb Tip Position: [4, -8, 239]\n",
      "Thumb Tip Position: [4, 4, 237]\n",
      "Thumb Tip Position: [4, 14, 233]\n",
      "Thumb Tip Position: [4, 48, 215]\n",
      "Thumb Tip Position: [4, 36, 219]\n",
      "Thumb Tip Position: [4, 53, 218]\n",
      "Thumb Tip Position: [4, 2, 229]\n",
      "Thumb Tip Position: [4, 13, 223]\n",
      "Thumb Tip Position: [4, 48, 221]\n",
      "Thumb Tip Position: [4, 57, 210]\n",
      "Thumb Tip Position: [4, 59, 207]\n",
      "Thumb Tip Position: [4, 58, 208]\n",
      "Thumb Tip Position: [4, 144, 226]\n",
      "Thumb Tip Position: [4, 230, 218]\n",
      "Thumb Tip Position: [4, 237, 214]\n",
      "Thumb Tip Position: [4, 235, 216]\n",
      "Thumb Tip Position: [4, 230, 214]\n",
      "Thumb Tip Position: [4, 220, 212]\n",
      "Thumb Tip Position: [4, 217, 215]\n",
      "Thumb Tip Position: [4, 219, 212]\n",
      "Thumb Tip Position: [4, 223, 213]\n",
      "Thumb Tip Position: [4, 229, 211]\n",
      "Thumb Tip Position: [4, 234, 213]\n",
      "Thumb Tip Position: [4, 235, 214]\n",
      "Thumb Tip Position: [4, 234, 215]\n",
      "Thumb Tip Position: [4, 212, 203]\n",
      "Thumb Tip Position: [4, 88, 213]\n",
      "Thumb Tip Position: [4, 152, 200]\n",
      "Thumb Tip Position: [4, 185, 198]\n",
      "Thumb Tip Position: [4, 201, 199]\n",
      "Thumb Tip Position: [4, 205, 204]\n",
      "Thumb Tip Position: [4, 207, 204]\n",
      "Thumb Tip Position: [4, 206, 203]\n",
      "Thumb Tip Position: [4, 204, 204]\n",
      "Thumb Tip Position: [4, 204, 203]\n",
      "Thumb Tip Position: [4, 217, 208]\n",
      "Thumb Tip Position: [4, 225, 209]\n",
      "Thumb Tip Position: [4, 229, 212]\n",
      "Thumb Tip Position: [4, 229, 213]\n",
      "Thumb Tip Position: [4, 239, 215]\n",
      "Thumb Tip Position: [4, 240, 219]\n",
      "Thumb Tip Position: [4, 243, 219]\n",
      "Thumb Tip Position: [4, 295, 257]\n",
      "Thumb Tip Position: [4, 305, 274]\n",
      "Thumb Tip Position: [4, 267, 246]\n",
      "Thumb Tip Position: [4, 242, 224]\n",
      "Thumb Tip Position: [4, 167, 192]\n",
      "Thumb Tip Position: [4, -12, 218]\n",
      "Thumb Tip Position: [4, -21, 219]\n",
      "Thumb Tip Position: [4, -25, 220]\n",
      "Thumb Tip Position: [4, -23, 224]\n",
      "Thumb Tip Position: [4, -27, 220]\n",
      "Thumb Tip Position: [4, -12, 204]\n",
      "Thumb Tip Position: [4, 1, 197]\n",
      "Thumb Tip Position: [4, -23, 205]\n",
      "Thumb Tip Position: [4, 8, 216]\n",
      "Thumb Tip Position: [4, 18, 214]\n",
      "Thumb Tip Position: [4, -2, 214]\n",
      "Thumb Tip Position: [4, -10, 206]\n",
      "Thumb Tip Position: [4, 179, 211]\n",
      "Thumb Tip Position: [4, 446, 342]\n",
      "Thumb Tip Position: [4, 458, 340]\n",
      "Thumb Tip Position: [4, 454, 328]\n",
      "Thumb Tip Position: [4, 456, 307]\n",
      "Thumb Tip Position: [4, 448, 323]\n",
      "Thumb Tip Position: [4, 451, 342]\n",
      "Thumb Tip Position: [4, 348, 268]\n",
      "Thumb Tip Position: [4, 312, 205]\n",
      "Thumb Tip Position: [4, 320, 241]\n",
      "Thumb Tip Position: [4, 337, 254]\n",
      "Thumb Tip Position: [4, 341, 254]\n",
      "Thumb Tip Position: [4, 338, 254]\n",
      "Thumb Tip Position: [4, 340, 251]\n",
      "Thumb Tip Position: [4, 338, 252]\n",
      "Thumb Tip Position: [4, 337, 252]\n",
      "Thumb Tip Position: [4, 333, 250]\n",
      "Thumb Tip Position: [4, 332, 250]\n",
      "Thumb Tip Position: [4, 331, 250]\n",
      "Thumb Tip Position: [4, 331, 249]\n",
      "Thumb Tip Position: [4, 330, 250]\n",
      "Thumb Tip Position: [4, 330, 249]\n",
      "Thumb Tip Position: [4, 330, 250]\n",
      "Thumb Tip Position: [4, 328, 249]\n",
      "Thumb Tip Position: [4, 329, 249]\n",
      "Thumb Tip Position: [4, 328, 248]\n",
      "Thumb Tip Position: [4, 328, 248]\n",
      "Thumb Tip Position: [4, 327, 248]\n",
      "Thumb Tip Position: [4, 326, 248]\n",
      "Thumb Tip Position: [4, 325, 248]\n",
      "Thumb Tip Position: [4, 325, 248]\n",
      "Thumb Tip Position: [4, 326, 248]\n",
      "Thumb Tip Position: [4, 325, 247]\n",
      "Thumb Tip Position: [4, 325, 248]\n",
      "Thumb Tip Position: [4, 324, 248]\n",
      "Thumb Tip Position: [4, 324, 247]\n",
      "Thumb Tip Position: [4, 324, 247]\n",
      "Thumb Tip Position: [4, 325, 248]\n",
      "Thumb Tip Position: [4, 324, 248]\n",
      "Thumb Tip Position: [4, 324, 248]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 323, 248]\n",
      "Thumb Tip Position: [4, 324, 247]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 322, 247]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 322, 247]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 324, 249]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 323, 248]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 323, 248]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 323, 248]\n",
      "Thumb Tip Position: [4, 323, 248]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 323, 248]\n",
      "Thumb Tip Position: [4, 323, 248]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 322, 247]\n",
      "Thumb Tip Position: [4, 322, 247]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 323, 246]\n",
      "Thumb Tip Position: [4, 322, 247]\n",
      "Thumb Tip Position: [4, 322, 247]\n",
      "Thumb Tip Position: [4, 323, 247]\n",
      "Thumb Tip Position: [4, 322, 248]\n",
      "Thumb Tip Position: [4, 322, 248]\n",
      "Thumb Tip Position: [4, 320, 247]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 245]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 320, 247]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 321, 246]\n",
      "Thumb Tip Position: [4, 320, 247]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 245]\n",
      "Thumb Tip Position: [4, 318, 245]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 245]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 245]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 321, 247]\n",
      "Thumb Tip Position: [4, 321, 247]\n",
      "Thumb Tip Position: [4, 321, 247]\n",
      "Thumb Tip Position: [4, 321, 247]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 320, 247]\n",
      "Thumb Tip Position: [4, 320, 247]\n",
      "Thumb Tip Position: [4, 320, 247]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 320, 247]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 321, 246]\n",
      "Thumb Tip Position: [4, 320, 247]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 320, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 245]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 318, 246]\n",
      "Thumb Tip Position: [4, 318, 246]\n",
      "Thumb Tip Position: [4, 318, 246]\n",
      "Thumb Tip Position: [4, 318, 246]\n",
      "Thumb Tip Position: [4, 318, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 318, 246]\n",
      "Thumb Tip Position: [4, 319, 245]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 318, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 318, 245]\n",
      "Thumb Tip Position: [4, 319, 245]\n",
      "Thumb Tip Position: [4, 319, 247]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 247]\n",
      "Thumb Tip Position: [4, 319, 245]\n",
      "Thumb Tip Position: [4, 320, 247]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 320, 247]\n",
      "Thumb Tip Position: [4, 320, 247]\n",
      "Thumb Tip Position: [4, 319, 247]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 320, 247]\n",
      "Thumb Tip Position: [4, 319, 247]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 319, 246]\n",
      "Thumb Tip Position: [4, 318, 245]\n",
      "Thumb Tip Position: [4, 318, 246]\n",
      "Thumb Tip Position: [4, 317, 246]\n",
      "Thumb Tip Position: [4, 316, 245]\n",
      "Thumb Tip Position: [4, 314, 245]\n",
      "Thumb Tip Position: [4, 313, 245]\n",
      "Thumb Tip Position: [4, 309, 256]\n",
      "Thumb Tip Position: [4, 317, 299]\n",
      "Thumb Tip Position: [4, 321, 334]\n",
      "Thumb Tip Position: [4, 345, 371]\n",
      "Thumb Tip Position: [4, 347, 318]\n",
      "Thumb Tip Position: [4, 330, 234]\n",
      "Thumb Tip Position: [4, 328, 188]\n",
      "Thumb Tip Position: [4, 321, 271]\n",
      "Thumb Tip Position: [4, 324, 315]\n",
      "Thumb Tip Position: [4, 322, 301]\n",
      "Thumb Tip Position: [4, 314, 314]\n",
      "Thumb Tip Position: [4, 318, 311]\n",
      "Thumb Tip Position: [4, 316, 310]\n",
      "Thumb Tip Position: [4, 325, 311]\n",
      "Thumb Tip Position: [4, 334, 315]\n",
      "Thumb Tip Position: [4, 341, 318]\n",
      "Thumb Tip Position: [4, 348, 317]\n",
      "Thumb Tip Position: [4, 332, 307]\n",
      "Thumb Tip Position: [4, 317, 295]\n",
      "Thumb Tip Position: [4, 298, 285]\n",
      "Thumb Tip Position: [4, 245, 260]\n",
      "Thumb Tip Position: [4, 237, 264]\n",
      "Thumb Tip Position: [4, 235, 260]\n",
      "Thumb Tip Position: [4, 237, 257]\n",
      "Thumb Tip Position: [4, 246, 257]\n",
      "Thumb Tip Position: [4, 260, 265]\n",
      "Thumb Tip Position: [4, 275, 271]\n",
      "Thumb Tip Position: [4, 288, 285]\n",
      "Thumb Tip Position: [4, 294, 296]\n",
      "Thumb Tip Position: [4, 286, 298]\n",
      "Thumb Tip Position: [4, 278, 294]\n",
      "Thumb Tip Position: [4, 272, 290]\n",
      "Thumb Tip Position: [4, 272, 290]\n",
      "Thumb Tip Position: [4, 278, 291]\n",
      "Thumb Tip Position: [4, 278, 291]\n",
      "Thumb Tip Position: [4, 272, 289]\n",
      "Thumb Tip Position: [4, 273, 290]\n",
      "Thumb Tip Position: [4, 275, 291]\n",
      "Thumb Tip Position: [4, 276, 291]\n",
      "Thumb Tip Position: [4, 276, 289]\n",
      "Thumb Tip Position: [4, 275, 289]\n",
      "Thumb Tip Position: [4, 275, 290]\n",
      "Thumb Tip Position: [4, 275, 291]\n",
      "Thumb Tip Position: [4, 269, 288]\n",
      "Thumb Tip Position: [4, 245, 269]\n",
      "Thumb Tip Position: [4, 167, 217]\n",
      "Thumb Tip Position: [4, 149, 212]\n",
      "Thumb Tip Position: [4, 142, 206]\n",
      "Thumb Tip Position: [4, 148, 203]\n",
      "Thumb Tip Position: [4, 150, 202]\n",
      "Thumb Tip Position: [4, 203, 205]\n",
      "Thumb Tip Position: [4, 297, 218]\n",
      "Thumb Tip Position: [4, 322, 231]\n",
      "Thumb Tip Position: [4, 329, 239]\n",
      "Thumb Tip Position: [4, 318, 241]\n",
      "Thumb Tip Position: [4, 308, 243]\n",
      "Thumb Tip Position: [4, 310, 243]\n",
      "Thumb Tip Position: [4, 315, 245]\n",
      "Thumb Tip Position: [4, 319, 245]\n",
      "Thumb Tip Position: [4, 316, 250]\n",
      "Thumb Tip Position: [4, 309, 250]\n",
      "Thumb Tip Position: [4, 295, 246]\n",
      "Thumb Tip Position: [4, 288, 246]\n",
      "Thumb Tip Position: [4, 285, 245]\n",
      "Thumb Tip Position: [4, 288, 238]\n",
      "Thumb Tip Position: [4, 298, 241]\n",
      "Thumb Tip Position: [4, 370, 262]\n",
      "Thumb Tip Position: [4, 394, 271]\n",
      "Thumb Tip Position: [4, 416, 280]\n",
      "Thumb Tip Position: [4, 446, 297]\n",
      "Thumb Tip Position: [4, 454, 302]\n",
      "Thumb Tip Position: [4, 456, 305]\n",
      "Thumb Tip Position: [4, 461, 308]\n",
      "Thumb Tip Position: [4, 461, 310]\n",
      "Thumb Tip Position: [4, 463, 310]\n",
      "Thumb Tip Position: [4, 463, 312]\n",
      "Thumb Tip Position: [4, 452, 306]\n",
      "Thumb Tip Position: [4, 403, 276]\n",
      "Thumb Tip Position: [4, 382, 268]\n",
      "Thumb Tip Position: [4, 322, 233]\n",
      "Thumb Tip Position: [4, 314, 230]\n",
      "Thumb Tip Position: [4, 286, 213]\n",
      "Thumb Tip Position: [4, 286, 222]\n",
      "Thumb Tip Position: [4, 288, 222]\n",
      "Thumb Tip Position: [4, 288, 224]\n",
      "Thumb Tip Position: [4, 296, 235]\n",
      "Thumb Tip Position: [4, 296, 233]\n",
      "Thumb Tip Position: [4, 296, 233]\n",
      "Thumb Tip Position: [4, 296, 233]\n",
      "Thumb Tip Position: [4, 297, 232]\n",
      "Thumb Tip Position: [4, 295, 231]\n",
      "Thumb Tip Position: [4, 305, 234]\n",
      "Thumb Tip Position: [4, 317, 235]\n",
      "Thumb Tip Position: [4, 329, 236]\n",
      "Thumb Tip Position: [4, 334, 239]\n",
      "Thumb Tip Position: [4, 333, 240]\n",
      "Thumb Tip Position: [4, 338, 238]\n",
      "Thumb Tip Position: [4, 331, 238]\n",
      "Thumb Tip Position: [4, 336, 236]\n",
      "Thumb Tip Position: [4, 337, 236]\n",
      "Thumb Tip Position: [4, 337, 235]\n",
      "Thumb Tip Position: [4, 338, 235]\n",
      "Thumb Tip Position: [4, 339, 235]\n",
      "Thumb Tip Position: [4, 339, 233]\n",
      "Thumb Tip Position: [4, 338, 232]\n",
      "Thumb Tip Position: [4, 339, 232]\n",
      "Thumb Tip Position: [4, 338, 233]\n",
      "Thumb Tip Position: [4, 337, 233]\n",
      "Thumb Tip Position: [4, 336, 232]\n",
      "Thumb Tip Position: [4, 335, 232]\n",
      "Thumb Tip Position: [4, 336, 232]\n",
      "Thumb Tip Position: [4, 336, 233]\n",
      "Thumb Tip Position: [4, 333, 231]\n",
      "Thumb Tip Position: [4, 333, 232]\n",
      "Thumb Tip Position: [4, 332, 232]\n",
      "Thumb Tip Position: [4, 331, 232]\n",
      "Thumb Tip Position: [4, 328, 231]\n",
      "Thumb Tip Position: [4, 328, 230]\n",
      "Thumb Tip Position: [4, 327, 230]\n",
      "Thumb Tip Position: [4, 327, 229]\n",
      "Thumb Tip Position: [4, 326, 228]\n",
      "Thumb Tip Position: [4, 324, 227]\n",
      "Thumb Tip Position: [4, 322, 227]\n",
      "Thumb Tip Position: [4, 321, 225]\n",
      "Thumb Tip Position: [4, 321, 224]\n",
      "Thumb Tip Position: [4, 321, 224]\n",
      "Thumb Tip Position: [4, 311, 224]\n",
      "Thumb Tip Position: [4, 292, 222]\n",
      "Thumb Tip Position: [4, 294, 212]\n",
      "Thumb Tip Position: [4, 289, 216]\n",
      "Thumb Tip Position: [4, 287, 213]\n",
      "Thumb Tip Position: [4, 285, 210]\n",
      "Thumb Tip Position: [4, 282, 210]\n",
      "Thumb Tip Position: [4, 286, 207]\n",
      "Thumb Tip Position: [4, 277, 195]\n",
      "Thumb Tip Position: [4, 214, 186]\n",
      "Thumb Tip Position: [4, 89, 195]\n",
      "Thumb Tip Position: [4, 80, 200]\n",
      "Thumb Tip Position: [4, 88, 203]\n",
      "Thumb Tip Position: [4, 94, 207]\n",
      "Thumb Tip Position: [4, 98, 211]\n",
      "Thumb Tip Position: [4, 99, 213]\n",
      "Thumb Tip Position: [4, 100, 214]\n",
      "Thumb Tip Position: [4, 123, 216]\n",
      "Thumb Tip Position: [4, 201, 179]\n",
      "Thumb Tip Position: [4, 297, 206]\n",
      "Thumb Tip Position: [4, 312, 208]\n",
      "Thumb Tip Position: [4, 314, 207]\n",
      "Thumb Tip Position: [4, 315, 207]\n",
      "Thumb Tip Position: [4, 315, 205]\n",
      "Thumb Tip Position: [4, 314, 203]\n",
      "Thumb Tip Position: [4, 312, 202]\n",
      "Thumb Tip Position: [4, 312, 201]\n",
      "Thumb Tip Position: [4, 315, 200]\n",
      "Thumb Tip Position: [4, 312, 195]\n",
      "Thumb Tip Position: [4, 314, 197]\n",
      "Thumb Tip Position: [4, 313, 195]\n",
      "Thumb Tip Position: [4, 310, 194]\n",
      "Thumb Tip Position: [4, 310, 194]\n",
      "Thumb Tip Position: [4, 309, 193]\n",
      "Thumb Tip Position: [4, 306, 194]\n",
      "Thumb Tip Position: [4, 307, 192]\n",
      "Thumb Tip Position: [4, 322, 195]\n",
      "Thumb Tip Position: [4, 333, 204]\n",
      "Thumb Tip Position: [4, 340, 209]\n",
      "Thumb Tip Position: [4, 339, 212]\n",
      "Thumb Tip Position: [4, 335, 208]\n",
      "Thumb Tip Position: [4, 333, 208]\n",
      "Thumb Tip Position: [4, 337, 211]\n",
      "Thumb Tip Position: [4, 336, 211]\n",
      "Thumb Tip Position: [4, 334, 208]\n",
      "Thumb Tip Position: [4, 336, 207]\n",
      "Thumb Tip Position: [4, 341, 209]\n",
      "Thumb Tip Position: [4, 347, 216]\n",
      "Thumb Tip Position: [4, 346, 214]\n",
      "Thumb Tip Position: [4, 347, 216]\n",
      "Thumb Tip Position: [4, 346, 217]\n",
      "Thumb Tip Position: [4, 347, 218]\n",
      "Thumb Tip Position: [4, 351, 217]\n",
      "Thumb Tip Position: [4, 364, 226]\n",
      "Thumb Tip Position: [4, 374, 238]\n",
      "Thumb Tip Position: [4, 380, 246]\n",
      "Thumb Tip Position: [4, 381, 246]\n",
      "Thumb Tip Position: [4, 387, 255]\n",
      "Thumb Tip Position: [4, 388, 256]\n",
      "Thumb Tip Position: [4, 389, 266]\n",
      "Thumb Tip Position: [4, 392, 268]\n",
      "Thumb Tip Position: [4, 395, 270]\n",
      "Thumb Tip Position: [4, 397, 270]\n",
      "Thumb Tip Position: [4, 395, 273]\n",
      "Thumb Tip Position: [4, 396, 272]\n",
      "Thumb Tip Position: [4, 400, 278]\n",
      "Thumb Tip Position: [4, 400, 279]\n",
      "Thumb Tip Position: [4, 403, 281]\n",
      "Thumb Tip Position: [4, 402, 281]\n",
      "Thumb Tip Position: [4, 402, 279]\n",
      "Thumb Tip Position: [4, 404, 278]\n",
      "Thumb Tip Position: [4, 404, 278]\n",
      "Thumb Tip Position: [4, 403, 277]\n",
      "Thumb Tip Position: [4, 400, 278]\n",
      "Thumb Tip Position: [4, 401, 277]\n",
      "Thumb Tip Position: [4, 397, 277]\n",
      "Thumb Tip Position: [4, 392, 273]\n",
      "Thumb Tip Position: [4, 376, 253]\n",
      "Thumb Tip Position: [4, 370, 248]\n",
      "Thumb Tip Position: [4, 371, 246]\n",
      "Thumb Tip Position: [4, 360, 239]\n",
      "Thumb Tip Position: [4, 327, 217]\n",
      "Thumb Tip Position: [4, 324, 212]\n",
      "Thumb Tip Position: [4, 321, 211]\n",
      "Thumb Tip Position: [4, 314, 209]\n",
      "Thumb Tip Position: [4, 271, 204]\n",
      "Thumb Tip Position: [4, 257, 200]\n",
      "Thumb Tip Position: [4, 259, 198]\n",
      "Thumb Tip Position: [4, 255, 187]\n",
      "Thumb Tip Position: [4, 236, 189]\n",
      "Thumb Tip Position: [4, 227, 217]\n",
      "Thumb Tip Position: [4, 237, 225]\n",
      "Thumb Tip Position: [4, 228, 218]\n",
      "Thumb Tip Position: [4, 216, 217]\n",
      "Thumb Tip Position: [4, 224, 214]\n",
      "Thumb Tip Position: [4, 220, 218]\n",
      "Thumb Tip Position: [4, 215, 217]\n",
      "Thumb Tip Position: [4, 231, 220]\n",
      "Thumb Tip Position: [4, 235, 215]\n",
      "Thumb Tip Position: [4, 194, 202]\n",
      "Thumb Tip Position: [4, 164, 203]\n",
      "Thumb Tip Position: [4, 143, 210]\n",
      "Thumb Tip Position: [4, 137, 211]\n",
      "Thumb Tip Position: [4, 131, 213]\n",
      "Thumb Tip Position: [4, 130, 219]\n",
      "Thumb Tip Position: [4, 133, 218]\n",
      "Thumb Tip Position: [4, 121, 223]\n",
      "Thumb Tip Position: [4, 111, 230]\n",
      "Thumb Tip Position: [4, 108, 233]\n",
      "Thumb Tip Position: [4, 111, 232]\n",
      "Thumb Tip Position: [4, 117, 228]\n",
      "Thumb Tip Position: [4, 132, 230]\n",
      "Thumb Tip Position: [4, 186, 232]\n",
      "Thumb Tip Position: [4, 286, 211]\n",
      "Thumb Tip Position: [4, 352, 259]\n",
      "Thumb Tip Position: [4, 368, 256]\n",
      "Thumb Tip Position: [4, 384, 246]\n",
      "Thumb Tip Position: [4, 384, 245]\n",
      "Thumb Tip Position: [4, 385, 242]\n",
      "Thumb Tip Position: [4, 359, 221]\n",
      "Thumb Tip Position: [4, 316, 190]\n",
      "Thumb Tip Position: [4, 303, 188]\n",
      "Thumb Tip Position: [4, 308, 188]\n",
      "Thumb Tip Position: [4, 313, 188]\n",
      "Thumb Tip Position: [4, 346, 204]\n",
      "Thumb Tip Position: [4, 371, 221]\n",
      "Thumb Tip Position: [4, 374, 229]\n",
      "Thumb Tip Position: [4, 376, 231]\n",
      "Thumb Tip Position: [4, 376, 230]\n",
      "Thumb Tip Position: [4, 374, 229]\n",
      "Thumb Tip Position: [4, 376, 227]\n",
      "Thumb Tip Position: [4, 375, 229]\n",
      "Thumb Tip Position: [4, 379, 231]\n",
      "Thumb Tip Position: [4, 378, 233]\n",
      "Thumb Tip Position: [4, 360, 213]\n",
      "Thumb Tip Position: [4, 350, 211]\n",
      "Thumb Tip Position: [4, 328, 201]\n",
      "Thumb Tip Position: [4, 324, 204]\n",
      "Thumb Tip Position: [4, 318, 204]\n",
      "Thumb Tip Position: [4, 313, 210]\n",
      "Thumb Tip Position: [4, 312, 209]\n",
      "Thumb Tip Position: [4, 312, 208]\n",
      "Thumb Tip Position: [4, 309, 207]\n",
      "Thumb Tip Position: [4, 311, 206]\n",
      "Thumb Tip Position: [4, 310, 205]\n",
      "Thumb Tip Position: [4, 320, 204]\n",
      "Thumb Tip Position: [4, 343, 222]\n",
      "Thumb Tip Position: [4, 353, 227]\n",
      "Thumb Tip Position: [4, 368, 233]\n",
      "Thumb Tip Position: [4, 378, 243]\n",
      "Thumb Tip Position: [4, 390, 253]\n",
      "Thumb Tip Position: [4, 391, 253]\n",
      "Thumb Tip Position: [4, 390, 254]\n",
      "Thumb Tip Position: [4, 392, 257]\n",
      "Thumb Tip Position: [4, 390, 258]\n",
      "Thumb Tip Position: [4, 388, 258]\n",
      "Thumb Tip Position: [4, 370, 262]\n",
      "Thumb Tip Position: [4, 356, 257]\n",
      "Thumb Tip Position: [4, 322, 255]\n",
      "Thumb Tip Position: [4, 407, 387]\n",
      "Thumb Tip Position: [4, 408, 389]\n",
      "Thumb Tip Position: [4, 398, 378]\n",
      "Thumb Tip Position: [4, 406, 399]\n",
      "Thumb Tip Position: [4, 407, 400]\n",
      "Thumb Tip Position: [4, 419, 387]\n",
      "Thumb Tip Position: [4, 434, 443]\n",
      "Thumb Tip Position: [4, 430, 417]\n",
      "Thumb Tip Position: [4, 413, 380]\n",
      "Thumb Tip Position: [4, 426, 376]\n",
      "Thumb Tip Position: [4, 417, 413]\n",
      "Thumb Tip Position: [4, 431, 428]\n",
      "Thumb Tip Position: [4, 406, 380]\n",
      "Thumb Tip Position: [4, 400, 373]\n",
      "Thumb Tip Position: [4, 424, 394]\n",
      "Thumb Tip Position: [4, 497, 328]\n",
      "Thumb Tip Position: [4, 474, 186]\n",
      "Thumb Tip Position: [4, 410, 196]\n",
      "Thumb Tip Position: [4, 361, 181]\n",
      "Thumb Tip Position: [4, 351, 180]\n",
      "Thumb Tip Position: [4, 345, 190]\n",
      "Thumb Tip Position: [4, 333, 193]\n",
      "Thumb Tip Position: [4, 311, 186]\n",
      "Thumb Tip Position: [4, 317, 185]\n",
      "Thumb Tip Position: [4, 356, 214]\n",
      "Thumb Tip Position: [4, 353, 213]\n",
      "Thumb Tip Position: [4, 238, 150]\n",
      "Thumb Tip Position: [4, 129, 151]\n",
      "Thumb Tip Position: [4, 138, 160]\n",
      "Thumb Tip Position: [4, 200, 149]\n",
      "Thumb Tip Position: [4, 166, 145]\n",
      "Thumb Tip Position: [4, 477, 254]\n",
      "Thumb Tip Position: [4, 497, 269]\n",
      "Thumb Tip Position: [4, 495, 281]\n",
      "Thumb Tip Position: [4, 491, 284]\n",
      "Thumb Tip Position: [4, 499, 282]\n",
      "Thumb Tip Position: [4, 492, 291]\n",
      "Thumb Tip Position: [4, 486, 300]\n",
      "Thumb Tip Position: [4, 475, 309]\n",
      "Thumb Tip Position: [4, 473, 311]\n",
      "Thumb Tip Position: [4, 483, 302]\n",
      "Thumb Tip Position: [4, 485, 300]\n",
      "Thumb Tip Position: [4, 483, 300]\n",
      "Thumb Tip Position: [4, 478, 303]\n",
      "Thumb Tip Position: [4, 450, 320]\n",
      "Thumb Tip Position: [4, 457, 311]\n",
      "Thumb Tip Position: [4, 565, 241]\n",
      "Thumb Tip Position: [4, 334, 203]\n",
      "Thumb Tip Position: [4, 350, 210]\n",
      "Thumb Tip Position: [4, 355, 213]\n",
      "Thumb Tip Position: [4, 356, 217]\n",
      "Thumb Tip Position: [4, 361, 217]\n",
      "Thumb Tip Position: [4, 349, 214]\n",
      "Thumb Tip Position: [4, 335, 204]\n",
      "Thumb Tip Position: [4, 341, 206]\n",
      "Thumb Tip Position: [4, 331, 207]\n",
      "Thumb Tip Position: [4, 257, 172]\n",
      "Thumb Tip Position: [4, 448, 321]\n",
      "Thumb Tip Position: [4, 454, 317]\n",
      "Thumb Tip Position: [4, 452, 313]\n",
      "Thumb Tip Position: [4, 458, 306]\n",
      "Thumb Tip Position: [4, 458, 314]\n",
      "Thumb Tip Position: [4, 455, 315]\n",
      "Thumb Tip Position: [4, 449, 320]\n",
      "Thumb Tip Position: [4, 446, 324]\n",
      "Thumb Tip Position: [4, 448, 322]\n",
      "Thumb Tip Position: [4, 447, 324]\n",
      "Thumb Tip Position: [4, 428, 337]\n",
      "Thumb Tip Position: [4, 380, 374]\n",
      "Thumb Tip Position: [4, 388, 366]\n",
      "Thumb Tip Position: [4, 385, 368]\n",
      "Thumb Tip Position: [4, 385, 369]\n",
      "Thumb Tip Position: [4, 390, 366]\n",
      "Thumb Tip Position: [4, 398, 357]\n",
      "Thumb Tip Position: [4, 397, 358]\n",
      "Thumb Tip Position: [4, 393, 358]\n",
      "Thumb Tip Position: [4, 395, 357]\n",
      "Thumb Tip Position: [4, 392, 361]\n",
      "Thumb Tip Position: [4, 393, 362]\n",
      "Thumb Tip Position: [4, 394, 360]\n",
      "Thumb Tip Position: [4, 394, 361]\n",
      "Thumb Tip Position: [4, 397, 359]\n",
      "Thumb Tip Position: [4, 399, 357]\n",
      "Thumb Tip Position: [4, 398, 356]\n",
      "Thumb Tip Position: [4, 60, 196]\n",
      "Thumb Tip Position: [4, 60, 202]\n",
      "Thumb Tip Position: [4, 58, 204]\n",
      "Thumb Tip Position: [4, 56, 202]\n",
      "Thumb Tip Position: [4, 62, 199]\n",
      "Thumb Tip Position: [4, 61, 201]\n",
      "Thumb Tip Position: [4, 60, 201]\n",
      "Thumb Tip Position: [4, 60, 202]\n",
      "Thumb Tip Position: [4, 61, 204]\n",
      "Thumb Tip Position: [4, 61, 201]\n",
      "Thumb Tip Position: [4, 64, 202]\n",
      "Thumb Tip Position: [4, 156, 206]\n",
      "Thumb Tip Position: [4, 173, 191]\n",
      "Thumb Tip Position: [4, 172, 186]\n",
      "Thumb Tip Position: [4, 157, 179]\n",
      "Thumb Tip Position: [4, 155, 182]\n",
      "Thumb Tip Position: [4, 174, 190]\n",
      "Thumb Tip Position: [4, 194, 220]\n",
      "Thumb Tip Position: [4, 197, 220]\n",
      "Thumb Tip Position: [4, 209, 219]\n",
      "Thumb Tip Position: [4, 264, 209]\n",
      "Thumb Tip Position: [4, 273, 209]\n",
      "Thumb Tip Position: [4, 271, 213]\n",
      "Thumb Tip Position: [4, 274, 212]\n",
      "Thumb Tip Position: [4, 274, 213]\n",
      "Thumb Tip Position: [4, 276, 213]\n",
      "Thumb Tip Position: [4, 275, 213]\n",
      "Thumb Tip Position: [4, 274, 212]\n",
      "Thumb Tip Position: [4, 275, 212]\n",
      "Thumb Tip Position: [4, 272, 210]\n",
      "Thumb Tip Position: [4, 272, 210]\n",
      "Thumb Tip Position: [4, 273, 210]\n",
      "Thumb Tip Position: [4, 273, 210]\n",
      "Thumb Tip Position: [4, 270, 209]\n",
      "Thumb Tip Position: [4, 266, 207]\n",
      "Thumb Tip Position: [4, 264, 205]\n",
      "Thumb Tip Position: [4, 273, 204]\n",
      "Thumb Tip Position: [4, 338, 281]\n",
      "Thumb Tip Position: [4, 382, 334]\n",
      "Thumb Tip Position: [4, 390, 342]\n",
      "Thumb Tip Position: [4, 394, 344]\n",
      "Thumb Tip Position: [4, 408, 358]\n",
      "Thumb Tip Position: [4, 421, 358]\n",
      "Thumb Tip Position: [4, 419, 348]\n",
      "Thumb Tip Position: [4, 410, 347]\n",
      "Thumb Tip Position: [4, 346, 332]\n",
      "Thumb Tip Position: [4, 299, 328]\n",
      "Thumb Tip Position: [4, 266, 320]\n",
      "Thumb Tip Position: [4, 246, 315]\n",
      "Thumb Tip Position: [4, 255, 323]\n",
      "Thumb Tip Position: [4, 262, 322]\n",
      "Thumb Tip Position: [4, 268, 317]\n",
      "Thumb Tip Position: [4, 274, 314]\n",
      "Thumb Tip Position: [4, 281, 308]\n",
      "Thumb Tip Position: [4, 277, 304]\n",
      "Thumb Tip Position: [4, 279, 304]\n",
      "Thumb Tip Position: [4, 280, 301]\n",
      "Thumb Tip Position: [4, 276, 309]\n",
      "Thumb Tip Position: [4, 280, 332]\n",
      "Thumb Tip Position: [4, 282, 338]\n",
      "Thumb Tip Position: [4, 283, 340]\n",
      "Thumb Tip Position: [4, 284, 340]\n",
      "Thumb Tip Position: [4, 284, 338]\n",
      "Thumb Tip Position: [4, 284, 319]\n",
      "Thumb Tip Position: [4, 269, 287]\n",
      "Thumb Tip Position: [4, 238, 249]\n",
      "Thumb Tip Position: [4, 205, 232]\n",
      "Thumb Tip Position: [4, 202, 226]\n",
      "Thumb Tip Position: [4, 202, 224]\n",
      "Thumb Tip Position: [4, 199, 227]\n",
      "Thumb Tip Position: [4, 210, 216]\n",
      "Thumb Tip Position: [4, 227, 211]\n",
      "Thumb Tip Position: [4, 229, 225]\n",
      "Thumb Tip Position: [4, 230, 226]\n",
      "Thumb Tip Position: [4, 226, 223]\n",
      "Thumb Tip Position: [4, 166, 194]\n",
      "Thumb Tip Position: [4, 141, 206]\n",
      "Thumb Tip Position: [4, 127, 202]\n",
      "Thumb Tip Position: [4, 132, 209]\n",
      "Thumb Tip Position: [4, 140, 220]\n",
      "Thumb Tip Position: [4, 150, 226]\n",
      "Thumb Tip Position: [4, 150, 211]\n",
      "Thumb Tip Position: [4, 101, 226]\n",
      "Thumb Tip Position: [4, 91, 229]\n",
      "Thumb Tip Position: [4, 93, 228]\n",
      "Thumb Tip Position: [4, 94, 230]\n",
      "Thumb Tip Position: [4, 104, 231]\n",
      "Thumb Tip Position: [4, 107, 232]\n",
      "Thumb Tip Position: [4, 123, 229]\n",
      "Thumb Tip Position: [4, 110, 231]\n",
      "Thumb Tip Position: [4, 104, 235]\n",
      "Thumb Tip Position: [4, 115, 230]\n",
      "Thumb Tip Position: [4, 143, 213]\n",
      "Thumb Tip Position: [4, 226, 201]\n",
      "Thumb Tip Position: [4, 247, 207]\n",
      "Thumb Tip Position: [4, 301, 219]\n",
      "Thumb Tip Position: [4, 354, 231]\n",
      "Thumb Tip Position: [4, 380, 252]\n",
      "Thumb Tip Position: [4, 383, 243]\n",
      "Thumb Tip Position: [4, 392, 259]\n",
      "Thumb Tip Position: [4, 373, 307]\n",
      "Thumb Tip Position: [4, 389, 289]\n",
      "Thumb Tip Position: [4, 405, 274]\n",
      "Thumb Tip Position: [4, 431, 282]\n",
      "Thumb Tip Position: [4, 427, 275]\n",
      "Thumb Tip Position: [4, 398, 327]\n",
      "Thumb Tip Position: [4, 368, 391]\n",
      "Thumb Tip Position: [4, 372, 380]\n",
      "Thumb Tip Position: [4, 437, 484]\n",
      "Thumb Tip Position: [4, 523, 348]\n",
      "Thumb Tip Position: [4, 313, 319]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "class HandTracker:\n",
    "    def __init__(self, mode=False, max_hands=2, detection_confidence=0.5, tracking_confidence=0.5):\n",
    "        \"\"\"\n",
    "        Initialize Hand Tracking\n",
    "        \n",
    "        Args:\n",
    "            mode (bool): Static mode or tracking mode\n",
    "            max_hands (int): Maximum number of hands to detect\n",
    "            detection_confidence (float): Minimum confidence for hand detection\n",
    "            tracking_confidence (float): Minimum confidence for hand tracking\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.max_hands = max_hands\n",
    "        self.detection_confidence = detection_confidence\n",
    "        self.tracking_confidence = tracking_confidence\n",
    "        \n",
    "        # MediaPipe Hand solutions\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=self.mode,\n",
    "            max_num_hands=self.max_hands,\n",
    "            min_detection_confidence=self.detection_confidence,\n",
    "            min_tracking_confidence=self.tracking_confidence\n",
    "        )\n",
    "        \n",
    "        # Drawing utilities\n",
    "        self.mp_draw = mp.solutions.drawing_utils\n",
    "        \n",
    "    def find_hands(self, frame, draw=True):\n",
    "        \"\"\"\n",
    "        Detect and track hands in the frame\n",
    "        \n",
    "        Args:\n",
    "            frame (numpy.ndarray): Input image/frame\n",
    "            draw (bool): Whether to draw hand landmarks\n",
    "        \n",
    "        Returns:\n",
    "            frame with or without hand landmarks\n",
    "            list of detected hands with their landmarks\n",
    "        \"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the frame and find hands\n",
    "        self.results = self.hands.process(frame_rgb)\n",
    "        \n",
    "        # Draw landmarks if hands are detected\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            for hand_landmarks in self.results.multi_hand_landmarks:\n",
    "                if draw:\n",
    "                    self.mp_draw.draw_landmarks(\n",
    "                        frame, \n",
    "                        hand_landmarks, \n",
    "                        self.mp_hands.HAND_CONNECTIONS\n",
    "                    )\n",
    "        \n",
    "        return frame, self.results.multi_hand_landmarks\n",
    "    \n",
    "    def find_hand_positions(self, frame, hand_number=0, draw=True):\n",
    "        \"\"\"\n",
    "        Get landmark positions for a specific hand\n",
    "        \n",
    "        Args:\n",
    "            frame (numpy.ndarray): Input image/frame\n",
    "            hand_number (int): Index of hand to analyze\n",
    "            draw (bool): Whether to draw landmark points\n",
    "        \n",
    "        Returns:\n",
    "            List of landmark positions\n",
    "        \"\"\"\n",
    "        landmarks_list = []\n",
    "        \n",
    "        if self.results.multi_hand_landmarks:\n",
    "            # Select the specified hand\n",
    "            my_hand = self.results.multi_hand_landmarks[hand_number]\n",
    "            \n",
    "            # Get height and width of frame\n",
    "            h, w, _ = frame.shape\n",
    "            \n",
    "            # Extract landmark coordinates\n",
    "            for id, landmark in enumerate(my_hand.landmark):\n",
    "                # Convert normalized coordinates to pixel coordinates\n",
    "                cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "                landmarks_list.append([id, cx, cy])\n",
    "                \n",
    "                # Optionally draw circles on landmarks\n",
    "                if draw:\n",
    "                    cv2.circle(frame, (cx, cy), 5, (255, 0, 255), cv2.FILLED)\n",
    "        \n",
    "        return landmarks_list\n",
    "\n",
    "def main():\n",
    "    # Video capture\n",
    "    cap = cv2.VideoCapture(0)  # 0 for default camera\n",
    "    \n",
    "    # Previous time for FPS calculation\n",
    "    prev_time = 0\n",
    "    current_time = 0\n",
    "    \n",
    "    # Initialize hand tracker\n",
    "    detector = HandTracker()\n",
    "    \n",
    "    while True:\n",
    "        # Read frame\n",
    "        success, frame = cap.read()\n",
    "        \n",
    "        # Find hands\n",
    "        frame, hands = detector.find_hands(frame)\n",
    "        \n",
    "        # Get landmark positions\n",
    "        if hands:\n",
    "            landmarks = detector.find_hand_positions(frame)\n",
    "            \n",
    "            # Example: Print position of specific landmarks\n",
    "            if landmarks:\n",
    "                # Print thumb tip position\n",
    "                print(f\"Thumb Tip Position: {landmarks[4]}\")\n",
    "        \n",
    "        # Calculate and display FPS\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "        \n",
    "        # Display FPS on frame\n",
    "        cv2.putText(\n",
    "            frame, \n",
    "            f'FPS: {int(fps)}', \n",
    "            (10, 70), \n",
    "            cv2.FONT_HERSHEY_PLAIN, \n",
    "            3, \n",
    "            (255, 0, 255), \n",
    "            3\n",
    "        )\n",
    "        \n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Hand Tracking\", frame)\n",
    "        \n",
    "        # Exit condition\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cfd573-2ed2-4c03-887a-f34dbb7f6273",
   "metadata": {},
   "source": [
    "# Code: Face Mesh Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b0c4db2-363b-4090-b20e-460d93fea673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as m\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default camera\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize Face Mesh with desired parameters\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,  # Detect only 1 face\n",
    "    refine_landmarks=True,  # Refine landmarks for features like lips and eyes\n",
    "    min_detection_confidence=0.5,  # Confidence threshold for detection\n",
    "    min_tracking_confidence=0.5  # Confidence threshold for tracking\n",
    ") as face_mesh:\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Cannot read the frame.\")\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB (MediaPipe requires RGB format)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame to detect face mesh\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "\n",
    "        # Draw face mesh on the frame\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # Draw landmarks on the face\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    face_landmarks,\n",
    "                    mp_face_mesh.FACEMESH_TESSELATION,  # Draw mesh connections\n",
    "                    landmark_drawing_spec=None,  # Default style\n",
    "                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "                )\n",
    "                # Optionally, add facial contours\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    face_landmarks,\n",
    "                    mp_face_mesh.FACEMESH_CONTOURS,  # Draw face contours\n",
    "                    landmark_drawing_spec=None,  # Default style\n",
    "                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style()\n",
    "                )\n",
    "\n",
    "        # Display the frame with face mesh\n",
    "        cv2.imshow(\"Face Mesh Detection\", frame)\n",
    "\n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756d71e-b4b3-4504-983a-1939a8c823ba",
   "metadata": {},
   "source": [
    "# Code: Iris Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba1c103a-7d49-488a-99ff-d5935250c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for default webcam\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize Face Mesh with refinement for iris tracking\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,  # Detect only 1 face\n",
    "    refine_landmarks=True,  # Enables iris detection\n",
    "    min_detection_confidence=0.5,  # Confidence threshold for detection\n",
    "    min_tracking_confidence=0.5  # Confidence threshold for tracking\n",
    ") as face_mesh:\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Cannot read the frame.\")\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB (MediaPipe requires RGB format)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame to detect face and iris\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "\n",
    "        # Draw face mesh and iris landmarks\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # Draw the full face mesh\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    face_landmarks,\n",
    "                    mp_face_mesh.FACEMESH_IRISES,  # Draw iris-specific landmarks\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style()\n",
    "                )\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow(\"Iris Detection\", frame)\n",
    "\n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c84c7c-684c-45ab-8d58-1751c9c4c944",
   "metadata": {},
   "source": [
    "# Code: Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78776d19-eb22-431f-8549-7f383ddc165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default camera\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize Pose estimation with desired parameters\n",
    "with mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,  # Confidence threshold for detecting the pose\n",
    "    min_tracking_confidence=0.5    # Confidence threshold for tracking pose landmarks\n",
    ") as pose:\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Cannot read the frame.\")\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB (MediaPipe requires RGB format)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame to detect pose landmarks\n",
    "        results = pose.process(frame_rgb)\n",
    "\n",
    "        # Draw pose landmarks on the frame\n",
    "        if results.pose_landmarks:\n",
    "            # Draw landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                results.pose_landmarks,\n",
    "                mp_pose.POSE_CONNECTIONS,  # Draw the connections between landmarks\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow(\"Pose Estimation\", frame)\n",
    "\n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d0715c-3b88-45f7-a332-7f765d96cfc1",
   "metadata": {},
   "source": [
    "# Code: Hair Segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5103880a-8c82-4068-976f-be0d71f29c28",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:/a/opencv-python/opencv-python/opencv/modules/highgui/src/precomp.hpp:156: error: (-215:Assertion failed) src_depth != CV_16F && src_depth != CV_32S in function 'convertToShow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Display the original frame and segmented frame\u001b[39;00m\n\u001b[0;32m     50\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Frame\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[1;32m---> 51\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHair Segmentation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhair_region\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Break the loop when 'q' is pressed\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:/a/opencv-python/opencv-python/opencv/modules/highgui/src/precomp.hpp:156: error: (-215:Assertion failed) src_depth != CV_16F && src_depth != CV_32S in function 'convertToShow'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Selfie Segmentation\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for default webcam\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the Selfie Segmentation model\n",
    "with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as selfie_segmentation:\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Cannot read the frame.\")\n",
    "            break\n",
    "\n",
    "        # Flip the frame horizontally for a mirrored view\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB (MediaPipe requires RGB format)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Perform segmentation\n",
    "        results = selfie_segmentation.process(frame_rgb)\n",
    "\n",
    "        # Get the mask for segmentation\n",
    "        mask = results.segmentation_mask\n",
    "\n",
    "        # Threshold the mask to isolate the person\n",
    "        condition = mask > 0.5  # Threshold for hair or head region\n",
    "\n",
    "        # Create a black background\n",
    "        background = np.zeros(frame.shape, dtype=np.uint8)\n",
    "\n",
    "        # Apply the mask to isolate the person\n",
    "        segmented_frame = np.where(condition[:, :, None], frame, background)\n",
    "\n",
    "        # Optional: Enhance the hair region (for demonstration purposes)\n",
    "        # This step isolates the head and applies an effect\n",
    "        hair_region = np.where(condition[:, :, None], (0, 0, 255), frame)  # Highlights hair in red\n",
    "\n",
    "        # Display the original frame and segmented frame\n",
    "        cv2.imshow(\"Original Frame\", frame)\n",
    "        cv2.imshow(\"Hair Segmentation\", hair_region)\n",
    "\n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5080a3-cdaa-4ce8-ba7d-658fc0270426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Selfie Segmentation\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for default webcam\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the Selfie Segmentation model\n",
    "with mp_selfie_segmentation.SelfieSegmentation(model_selection=1) as selfie_segmentation:\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Cannot read the frame.\")\n",
    "            break\n",
    "\n",
    "        # Flip the frame horizontally for a mirrored view\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB (MediaPipe requires RGB format)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Perform segmentation\n",
    "        results = selfie_segmentation.process(frame_rgb)\n",
    "\n",
    "        # Get the mask for segmentation\n",
    "        mask = results.segmentation_mask\n",
    "\n",
    "        # Threshold the mask to isolate the person\n",
    "        condition = mask > 0.5  # Threshold for hair or head region\n",
    "\n",
    "        # Create a black background\n",
    "        background = np.zeros(frame.shape, dtype=np.uint8)\n",
    "\n",
    "        # Apply the mask to isolate the person\n",
    "        segmented_frame = np.where(condition[:, :, None], frame, background)\n",
    "\n",
    "        # Optional: Enhance the hair region (for demonstration purposes)\n",
    "        # This step isolates the head and applies an effect\n",
    "        hair_region = np.where(condition[:, :, None], (0, 0, 255), frame)  # Highlights hair in red\n",
    "\n",
    "        # Convert hair_region to uint8 (required for display)\n",
    "        hair_region = np.array(hair_region, dtype=np.uint8)\n",
    "\n",
    "        # Display the original frame and segmented frame\n",
    "        cv2.imshow(\"Original Frame\", frame)\n",
    "        cv2.imshow(\"Hair Segmentation\", hair_region)\n",
    "\n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f1a9d-3e05-4163-964c-f36ffadd094a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ea5af69-3de1-4e51-8b76-32b0f897f2ed",
   "metadata": {},
   "source": [
    "# Box Tracking Using MediaPipe Hands and OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860c3a27-3a32-45ad-bfb6-05a06dc098ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the Hands model\n",
    "with mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5) as hands:\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Cannot read the frame.\")\n",
    "            break\n",
    "\n",
    "        # Flip the frame horizontally for a mirrored view\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB (MediaPipe requires RGB format)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame to detect hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Draw bounding box if a hand is detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Extract landmark coordinates for the bounding box\n",
    "                h, w, c = frame.shape\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                x_max = 0\n",
    "                y_max = 0\n",
    "\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    x_min = min(x_min, x)\n",
    "                    y_min = min(y_min, y)\n",
    "                    x_max = max(x_max, x)\n",
    "                    y_max = max(y_max, y)\n",
    "\n",
    "                # Draw the bounding box\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw hand landmarks\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow(\"Box Tracking\", frame)\n",
    "\n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e33f12-f8f9-4eee-812f-ef0cad063506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the Hands model\n",
    "with mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5) as hands:\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Cannot read the frame.\")\n",
    "            break\n",
    "\n",
    "        # Flip the frame horizontally for a mirrored view\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the frame to RGB (MediaPipe requires RGB format)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame to detect hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Draw bounding box if a hand is detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Extract landmark coordinates for the bounding box\n",
    "                h, w, c = frame.shape\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                x_max = 0\n",
    "                y_max = 0\n",
    "\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    x_min = min(x_min, x)\n",
    "                    y_min = min(y_min, y)\n",
    "                    x_max = max(x_max, x)\n",
    "                    y_max = max(y_max, y)\n",
    "\n",
    "                # Draw the bounding box\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw hand landmarks\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow(\"Box Tracking\", frame)\n",
    "\n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfa389b-0716-430f-9a67-97343ea4aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands for tracking\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils  # For visualizing landmarks\n",
    "\n",
    "# Open webcam (use 0 for default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot access the webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize MediaPipe Hands model\n",
    "with mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5) as hands:\n",
    "    while True:\n",
    "        # Capture frame-by-frame from webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Cannot read the frame.\")\n",
    "            break\n",
    "\n",
    "        # Flip and convert the frame to RGB (required by MediaPipe)\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame for hand detection\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # If a hand is detected, draw bounding box\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                h, w, c = frame.shape  # Get frame dimensions\n",
    "                x_min, y_min, x_max, y_max = w, h, 0, 0\n",
    "\n",
    "                # Calculate bounding box coordinates from landmarks\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    x_min = min(x_min, x)\n",
    "                    y_min = min(y_min, y)\n",
    "                    x_max = max(x_max, x)\n",
    "                    y_max = max(y_max, y)\n",
    "\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "                # Optional: Draw landmarks and connections\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Box Tracking\", frame)\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0e4e4-1422-4072-a942-65aae38d11df",
   "metadata": {},
   "source": [
    "# Object Detection code  Using OpenCV and MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c23d497a-3267-420c-9401-ea550bc57822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fb314ee-b036-4044-aefe-9f55331ce268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model to C:\\Users\\Mega Computers\\AppData\\Roaming\\Python\\Python38\\site-packages\\mediapipe/modules/objectron/object_detection_3d_sneakers.tflite\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'mediapipe.python.solutions.drawing_utils' has no attribute 'draw_bbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 145\u001b[0m\n\u001b[0;32m    142\u001b[0m     detector\u001b[38;5;241m.\u001b[39mdetect_from_video()\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 142\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    139\u001b[0m detector \u001b[38;5;241m=\u001b[39m ObjectDetector()\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Start real-time detection from webcam\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_from_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 123\u001b[0m, in \u001b[0;36mObjectDetector.detect_from_video\u001b[1;34m(self, video_source, show_preview)\u001b[0m\n\u001b[0;32m    120\u001b[0m detection_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetect_objects(frame)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Draw detection results\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m result_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_detection_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetection_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_preview:\n",
      "Cell \u001b[1;32mIn[11], line 81\u001b[0m, in \u001b[0;36mObjectDetector.draw_detection_results\u001b[1;34m(self, image, detection_results)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mdetected_objects:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m detected_obj \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mdetected_objects:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;66;03m# Draw 3D bounding box\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m         \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolutions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrawing_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_bbox\u001b[49m(\n\u001b[0;32m     82\u001b[0m             image, \n\u001b[0;32m     83\u001b[0m             detected_obj\u001b[38;5;241m.\u001b[39mtranslation, \n\u001b[0;32m     84\u001b[0m             detected_obj\u001b[38;5;241m.\u001b[39mrotation, \n\u001b[0;32m     85\u001b[0m             detected_obj\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m     86\u001b[0m         )\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;66;03m# Optional: Add label\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Detected\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'mediapipe.python.solutions.drawing_utils' has no attribute 'draw_bbox'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "class ObjectDetector:\n",
    "    def __init__(self, confidence_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the object detection model using MediaPipe\n",
    "        \n",
    "        Args:\n",
    "            confidence_threshold (float): Minimum confidence threshold for detection\n",
    "        \"\"\"\n",
    "        # MediaPipe object detection setup\n",
    "        self.mp_detector = mp.solutions.objectron\n",
    "        \n",
    "        # Configure detection models\n",
    "        self.cup_detector = self.mp_detector.Objectron(\n",
    "            static_image_mode=False,\n",
    "            max_num_objects=5,\n",
    "            min_detection_confidence=confidence_threshold,\n",
    "            model_name='Cup'\n",
    "        )\n",
    "        \n",
    "        self.shoe_detector = self.mp_detector.Objectron(\n",
    "            static_image_mode=False,\n",
    "            max_num_objects=5,\n",
    "            min_detection_confidence=confidence_threshold,\n",
    "            model_name='Shoe'\n",
    "        )\n",
    "        \n",
    "        self.chair_detector = self.mp_detector.Objectron(\n",
    "            static_image_mode=False,\n",
    "            max_num_objects=5,\n",
    "            min_detection_confidence=confidence_threshold,\n",
    "            model_name='Chair'\n",
    "        )\n",
    "    \n",
    "    def detect_objects(self, image):\n",
    "        \"\"\"\n",
    "        Detect objects in the given image\n",
    "        \n",
    "        Args:\n",
    "            image (numpy.ndarray): Input image for object detection\n",
    "        \n",
    "        Returns:\n",
    "            dict: Detected objects with their details\n",
    "        \"\"\"\n",
    "        # Convert image to RGB (MediaPipe requirement)\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect objects\n",
    "        results = {\n",
    "            'cups': self.cup_detector.process(rgb_image),\n",
    "            'shoes': self.shoe_detector.process(rgb_image),\n",
    "            'chairs': self.chair_detector.process(rgb_image)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def draw_detection_results(self, image, detection_results):\n",
    "        \"\"\"\n",
    "        Draw 3D bounding boxes for detected objects\n",
    "        \n",
    "        Args:\n",
    "            image (numpy.ndarray): Original input image\n",
    "            detection_results (dict): Object detection results\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Image with detection results visualized\n",
    "        \"\"\"\n",
    "        # Draw detections for each object type\n",
    "        object_types = ['cups', 'shoes', 'chairs']\n",
    "        colors = [(0, 255, 0), (255, 0, 0), (0, 0, 255)]  # Green, Blue, Red\n",
    "        \n",
    "        for obj_type, color in zip(object_types, colors):\n",
    "            results = detection_results[obj_type]\n",
    "            \n",
    "            if results.detected_objects:\n",
    "                for detected_obj in results.detected_objects:\n",
    "                    # Draw 3D bounding box\n",
    "                    mp.solutions.drawing_utils.draw_bbox(\n",
    "                        image, \n",
    "                        detected_obj.translation, \n",
    "                        detected_obj.rotation, \n",
    "                        detected_obj.scale\n",
    "                    )\n",
    "                    \n",
    "                    # Optional: Add label\n",
    "                    label = f\"{obj_type.capitalize()} Detected\"\n",
    "                    cv2.putText(\n",
    "                        image, \n",
    "                        label, \n",
    "                        (int(detected_obj.translation[0]), int(detected_obj.translation[1]) - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.9, \n",
    "                        color, \n",
    "                        2\n",
    "                    )\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def detect_from_video(self, video_source=0, show_preview=True):\n",
    "        \"\"\"\n",
    "        Perform real-time object detection from video source\n",
    "        \n",
    "        Args:\n",
    "            video_source (int/str): Video capture source (0 for webcam)\n",
    "            show_preview (bool): Whether to display detection results\n",
    "        \"\"\"\n",
    "        # Open video capture\n",
    "        cap = cv2.VideoCapture(video_source)\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            # Read frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Detect objects\n",
    "            detection_results = self.detect_objects(frame)\n",
    "            \n",
    "            # Draw detection results\n",
    "            result_image = self.draw_detection_results(frame, detection_results)\n",
    "            \n",
    "            # Display results\n",
    "            if show_preview:\n",
    "                cv2.imshow('Object Detection', result_image)\n",
    "            \n",
    "            # Exit on 'q' key press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    # Initialize object detector\n",
    "    detector = ObjectDetector()\n",
    "    \n",
    "    # Start real-time detection from webcam\n",
    "    detector.detect_from_video()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b14f3-289e-442d-a687-b002d74608cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
