{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a3076d-479b-4595-a84c-72a1c2baea61",
   "metadata": {},
   "source": [
    "# Grabcut Algorithm for Background Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c35b16d-2d56-4111-b975-f440c1157e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the input image\n",
    "image = cv2.imread('download (3).jpg')\n",
    "\n",
    "# Create a copy for displaying results later\n",
    "result_image = image.copy()\n",
    "\n",
    "# Define the rectangle (start_x, start_y, width, height) for the object\n",
    "rect = (50, 50, 400, 400)  # Adjust values based on your image\n",
    "\n",
    "# Create mask and temporary arrays used by GrabCut\n",
    "mask = np.zeros(image.shape[:2], dtype=np.uint8)  # Initial mask\n",
    "bgd_model = np.zeros((1, 65), dtype=np.float64)  # Background model\n",
    "fgd_model = np.zeros((1, 65), dtype=np.float64)  # Foreground model\n",
    "\n",
    "# Apply GrabCut\n",
    "cv2.grabCut(image, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# Modify mask: 0 and 2 are background, 1 and 3 are foreground\n",
    "mask_result = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "\n",
    "# Extract the foreground from the image\n",
    "foreground = image * mask_result[:, :, np.newaxis]\n",
    "\n",
    "# Replace background (e.g., with white background)\n",
    "background = np.ones_like(image, dtype=np.uint8) * 255  # White background\n",
    "combined = np.where(mask_result[:, :, np.newaxis] == 1, image, background)\n",
    "\n",
    "# Display results\n",
    "cv2.imshow('Original Image', result_image)\n",
    "cv2.imshow('Extracted Foreground', foreground)\n",
    "cv2.imshow('Background Changed Image', combined)\n",
    "\n",
    "# Wait for a key press and close all windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03bdfed-35a4-4d92-9767-4a5c7af82839",
   "metadata": {},
   "source": [
    "# Video Background Removal Using Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93a7e6-2ef5-4a1d-8a38-31919c09047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d09ac64e-2c13-497d-b339-8c7f0f4d102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"mixkit-man-and-woman-jogging-together-on-the-street-40881-hd-ready.mp4\")\n",
    "\n",
    "algo1 = cv2.createBackgroundSubtractorKNN(detectShadows=True)\n",
    "algo2 = cv2.createBackgroundSubtractorMOG2(detectShadows=True)\n",
    "\n",
    "while True:\n",
    "    r ,f = cap.read()\n",
    "    if r == True:\n",
    "        f = cv2.resize(f,(500, 400))\n",
    "        r1 = algo1.apply(f)\n",
    "        r2 = algo2.apply(f)\n",
    "        cv2.imshow(\"algo1\",r1)\n",
    "        cv2.imshow(\"algo2\",r2)\n",
    "        cv2.imshow(\"video \", f)\n",
    "        if cv2.waitKey(25) & 0xff == ord(\"p\"):\n",
    "            break\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "176ceafc-dc86-49a5-905d-adafa47b4852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the video\n",
    "video_path = 'mixkit-man-and-woman-jogging-together-on-the-street-40881-hd-ready.mp4'  # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Initialize the Background Subtractor\n",
    "background_subtractor = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=True)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize the frame for faster processing (optional)\n",
    "    frame = cv2.resize(frame, (640, 360))\n",
    "\n",
    "    # Apply the background subtractor\n",
    "    mask = background_subtractor.apply(frame)\n",
    "\n",
    "    # Remove shadows (optional)\n",
    "    _, mask = cv2.threshold(mask, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Extract the foreground\n",
    "    foreground = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "    # Create a white background\n",
    "    background = np.ones_like(frame, dtype=np.uint8) * 255\n",
    "    combined = np.where(mask[:, :, np.newaxis] == 255, frame, background)\n",
    "\n",
    "    # Display the frames\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Foreground', foreground)\n",
    "    cv2.imshow('Background Removed', combined)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de658067-196d-4150-877e-3575789038f1",
   "metadata": {},
   "source": [
    "# Object Tracking and Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfbe727c-7b32-4480-9c8c-bf3d50af1e3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: yolov4.cfg in function 'cv::dnn::dnn4_v20240521::readNetFromDarknet'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load YOLO model and classes\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov4.weights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov4.cfg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoco.names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: yolov4.cfg in function 'cv::dnn::dnn4_v20240521::readNetFromDarknet'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load YOLO model and classes\n",
    "net = cv2.dnn.readNet('yolov4.weights', 'yolov4.cfg')\n",
    "classes = []\n",
    "with open('coco.names', 'r') as f:\n",
    "    classes = f.read().splitlines()\n",
    "\n",
    "video = cv2.VideoCapture('mixkit-man-and-woman-jogging-together-on-the-street-40881-hd-ready.mp4')\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Prepare frame for YOLO\n",
    "    height, width = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (416, 416), (0, 0, 0), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Get YOLO output\n",
    "    layer_names = net.getUnconnectedOutLayersNames()\n",
    "    outputs = net.forward(layer_names)\n",
    "    \n",
    "    # Draw detections\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                box = detection[:4] * np.array([width, height, width, height])\n",
    "                (center_x, center_y, w, h) = box.astype(\"int\")\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, classes[class_id], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"YOLO Detection\", frame)\n",
    "    \n",
    "    # Exit on 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f40644-e2f8-4a06-82b4-9eb2f9dfbd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(\"mixkit-man-and-woman-jogging-together-on-the-street-40881-hd-ready.mp4\")\n",
    "\n",
    "while True:\n",
    "    r ,f = cap.read()\n",
    "    if r == True:\n",
    "        f = cv2.resize(f,(500, 400))\n",
    "        cv2.imshow(\"video \", f)\n",
    "        if cv2.waitKey(25) & 0xff == ord(\"p\"):\n",
    "            break\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbaac8-9783-496e-a6b2-7442b35dfed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03d15b82-6c18-4b65-9ac1-a89226649c9b",
   "metadata": {},
   "source": [
    "# Corner Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2961380f-31ea-433e-a734-2258e3a3a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('WhatsApp Image 2024-08-30 at 10.03.11 PM.jpeg')\n",
    "image = cv2.resize(image , (500,500))\n",
    "\n",
    "gry = cv2.cvtColor(image , cv2.COLOR_BGR2GRAY)\n",
    "gry = np.float32(gry)\n",
    "\n",
    "res = cv2.cornerHarris(gry ,4,3,0.04)\n",
    "res = cv2.dilate(res , None)\n",
    "\n",
    "image[res>0.01*res.max()] = [0,0,255]\n",
    "\n",
    "cv2.imshow('Detect image corner ', image)\n",
    "# Wait for a key press and close all windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa369b-9a0f-4c50-a83b-fbb85769ee50",
   "metadata": {},
   "source": [
    "# Shi-Tomasi Corner Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc3cf5e-1719-4b64-84a9-39f5a0c8162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mega Computers\\AppData\\Local\\Temp\\ipykernel_23872\\3374979080.py:18: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
      "  corners = np.int0(corners)  # Convert corner coordinates to integers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(\"WhatsApp Image 2024-08-30 at 10.03.11 PM.jpeg\")\n",
    "image = cv2.resize(image , (500,500))\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Parameters for Shi-Tomasi Corner Detection\n",
    "max_corners = 50        # Maximum number of corners to detect\n",
    "quality_level = 0.01    # Minimum quality of corners (0-1 scale)\n",
    "min_distance = 10       # Minimum distance between corners\n",
    "\n",
    "# Detect corners using Shi-Tomasi\n",
    "corners = cv2.goodFeaturesToTrack(gray, max_corners, quality_level, min_distance)\n",
    "corners = np.int0(corners)  # Convert corner coordinates to integers\n",
    "\n",
    "# Draw the detected corners on the original image\n",
    "for corner in corners:\n",
    "    x, y = corner.ravel()  # Flatten the (x, y) coordinates\n",
    "    cv2.circle(image, (x, y), 5, (0, 255, 0), 2)  # Draw green circles at each corner\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow(\"Shi-Tomasi Corners\", image)\n",
    "cv2.waitKey(0)  # Wait for a key press\n",
    "cv2.destroyAllWindows()  # Close all OpenCV windows\n",
    "\n",
    "# Save the result (optional)\n",
    "cv2.imwrite(\"shi_tomasi_output.jpg\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285c071-c55e-4636-8828-0821c77e7b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
